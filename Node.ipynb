{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2, 'EPSILON': 0.5, 'ALPHA': 0.9, 'MCTS_SEARCHES': 1000, 'LearningRate': 0.001, 'TEMPERATURE': 0.8, 'BatchSize': 64, 'Iterations': 10, 'GamesPerIteration': 128, 'Epochs': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js314\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm.auto import *\n",
    "\n",
    "class OmokGame:\n",
    "    def __init__(self):\n",
    "        global device\n",
    "        self.board = torch.zeros(15, 15, dtype=torch.float, device=device, requires_grad=False)\n",
    "        self.turn = 1\n",
    "        self.move_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.zero_()\n",
    "        self.turn = 1\n",
    "        self.move_history = []\n",
    "    \n",
    "    def get_board(self):\n",
    "        return self.board\n",
    "    \n",
    "    def get_legal_moves(self):\n",
    "        global device\n",
    "        rtr = torch.ones(15 * 15, dtype=torch.float, device=device, requires_grad=False)\n",
    "        for move in self.move_history:\n",
    "            rtr[move[0] * 15 + move[1]] = 0\n",
    "        return rtr\n",
    "    \n",
    "    def get_turn(self):\n",
    "        return self.turn\n",
    "    \n",
    "    def makeMove(self, x, y):\n",
    "        if self.board[x, y] != 0:\n",
    "            return False\n",
    "        self.board[x, y] = self.turn\n",
    "        self.move_history.append((x, y))\n",
    "        self.turn = -self.turn\n",
    "        return True\n",
    "    \n",
    "    def checkWin(self):\n",
    "        lastMove = self.move_history[-1]\n",
    "        # Search fron last move\n",
    "        def UtilitySqCheck(x, y):\n",
    "            if x < 0 or x >= 15 or y < 0 or y >= 15:\n",
    "                return 0\n",
    "            return self.board[x, y]\n",
    "        \n",
    "        last_Player = self.board[lastMove[0], lastMove[1]]\n",
    "\n",
    "        dxl = [-1, 0, 1, -1, 1, -1, 0, 1]\n",
    "        dyl = [-1, -1, -1, 0, 0, 1, 1, 1]\n",
    "\n",
    "        for i in range(8):\n",
    "            count = 1\n",
    "            for j in range(1, 5):\n",
    "                if UtilitySqCheck(lastMove[0] + dxl[i] * j, lastMove[1] + dyl[i] * j) == last_Player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            for j in range(1, 5):\n",
    "                if UtilitySqCheck(lastMove[0] - dxl[i] * j, lastMove[1] - dyl[i] * j) == last_Player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "            # In this rule, only exactly 5 stones is considered a win\n",
    "            if count == 5:\n",
    "                return last_Player\n",
    "        return 0\n",
    "    \n",
    "    def play(self, x, y):\n",
    "        if not self.makeMove(x, y):\n",
    "            return False\n",
    "        if len(self.move_history) == 15 * 15:\n",
    "            return 2\n",
    "        return self.checkWin()\n",
    "    \n",
    "    def __str__(self):\n",
    "        ans = \"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.board[i, j] == 1:\n",
    "                    ans += \"X\"\n",
    "                elif self.board[i, j] == -1:\n",
    "                    ans += \"O\"\n",
    "                else:\n",
    "                    ans += \".\"\n",
    "            ans += \"\\n\"\n",
    "        return ans\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Project Nexus : Omok Game\"\n",
    "\n",
    "    def copy(self):\n",
    "        game = OmokGame()\n",
    "        game.board = self.board.clone()\n",
    "        game.turn = self.turn\n",
    "        game.move_history = self.move_history.copy()\n",
    "        return game\n",
    "\n",
    "from configf import config\n",
    "import configf\n",
    "device = configf.device\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, move, state, parent=None, prior=0):\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.state = state\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.score_sum = 0\n",
    "        self.prior = prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for move, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                new_state = self.state.copy()\n",
    "                x, y = move // 15, move % 15\n",
    "                new_state.makeMove(x, y)\n",
    "                self.children.append(Node(move, new_state, self, prob))\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def is_root(self):\n",
    "        return self.parent == None\n",
    "    \n",
    "    def select(self, policy):\n",
    "        global device\n",
    "        # Calculate ucb, vectorlized\n",
    "        # Assume policy sums up to 1\n",
    "        q_values = torch.tensor([child.score_sum/child.visits if child.visits!=0 else 0 for child in self.children], dtype=torch.float, requires_grad=False, device=device)\n",
    "        # print(f\"Q value shape : {q_values.shape}\")\n",
    "        ucb = q_values + config['C'] * \\\n",
    "            (self.visits ** 0.5) / \\\n",
    "                (1 + torch.tensor([child.visits for child in self.children], dtype=torch.float, requires_grad=False, device=device))\\\n",
    "            * torch.tensor([child.prior for child in self.children], dtype=torch.float, requires_grad=False, device=device)\n",
    "        return self.children[torch.argmax(ucb)]\n",
    "    \n",
    "    def backpropagate(self, score):\n",
    "        self.visits += 1\n",
    "        self.score_sum += score\n",
    "        if not self.is_root():\n",
    "            self.parent.backpropagate(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def search(self, state):\n",
    "        global device\n",
    "        model.eval()\n",
    "\n",
    "        # First, use dirichlet noise to add some randomness to the root node\n",
    "        root = Node(None, state)\n",
    "\n",
    "        bd = torch.Tensor(state.get_board()).to(device).view(1, 1, 15, 15)\n",
    "        actor, critic = model(bd)\n",
    "\n",
    "        policy = torch.softmax(actor, dim=1).squeeze(0).detach().cpu().numpy()\n",
    "        policy = (1 - config['EPSILON']) * policy + config['EPSILON'] * np.random.dirichlet([config['ALPHA']] * 225)\n",
    "\n",
    "\n",
    "        valid_moves = state.get_legal_moves().cpu().numpy()\n",
    "        policy = policy * valid_moves\n",
    "        policy = policy / policy.sum()\n",
    "\n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(config['MCTS_SEARCHES']):\n",
    "            node = root\n",
    "            while not node.is_leaf():\n",
    "                node = node.select(policy)\n",
    "            \n",
    "            # Check if node is terminal\n",
    "            gameResult = node.state.checkWin()\n",
    "            if gameResult != 0:\n",
    "                node.backpropagate(gameResult)\n",
    "                continue\n",
    "            if gameResult == 0:\n",
    "                actor, critic = model(torch.Tensor(node.state.get_board()).to(device).view(1, 1, 15, 15))\n",
    "                policy = torch.softmax(actor, dim=1).squeeze(0).detach().cpu().numpy()\n",
    "                valid_moves = node.state.get_legal_moves().cpu().numpy()\n",
    "                policy = policy * valid_moves\n",
    "                policy = policy / policy.sum()\n",
    "\n",
    "                value = critic.item()\n",
    "\n",
    "                node.expand(policy)\n",
    "                node.backpropagate(value)\n",
    "        \n",
    "        action_probs = torch.zeros(225, dtype=torch.float, requires_grad=False, device=device)\n",
    "        for child in root.children:\n",
    "            action_probs[child.move] = child.visits\n",
    "        action_probs = action_probs / action_probs.sum()\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Nexus_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nexus_Small, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.Actor = nn.Linear(64, 225)\n",
    "        self.Critic = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(-1, 64)\n",
    "        return self.Actor(x), F.tanh(self.Critic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22_15-46-24\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def DateTimeGet():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print(DateTimeGet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nexus_Agent:\n",
    "    def __init__(self, model, optimizer):\n",
    "        global device\n",
    "        self.model = model\n",
    "        model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.mcts = MCTS(model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        board = OmokGame()\n",
    "\n",
    "        max_moves = 15 * 15\n",
    "        move_count = 0\n",
    "\n",
    "        while move_count < max_moves:\n",
    "            action_probs = self.mcts.search(board)\n",
    "            memory.append((board.get_board().clone(), action_probs.copy(), board.get_turn()))\n",
    "\n",
    "            # Apply temperature\n",
    "            policy = action_probs ** (1 / config['TEMPERATURE'])\n",
    "            policy /= policy.sum()\n",
    "\n",
    "            # Select action\n",
    "            action = np.random.choice(len(policy), p=policy)\n",
    "            x, y = action // 15, action % 15\n",
    "\n",
    "            # Play the move\n",
    "            played = board.play(x, y)\n",
    "            move_count += 1\n",
    "\n",
    "            if played == 1 or played == -1:\n",
    "                whoWon = played\n",
    "                break\n",
    "            elif played == 2:\n",
    "                whoWon = 0  # Draw\n",
    "                break\n",
    "        else:\n",
    "            whoWon = 0  # Draw if max_moves reached\n",
    "\n",
    "        # Assign outcomes\n",
    "        returnMemory = []\n",
    "        for state, probs, turn in memory:\n",
    "            if whoWon == 0:\n",
    "                outcome = 0\n",
    "            else:\n",
    "                outcome = whoWon * turn\n",
    "            returnMemory.append((state.clone(), probs.copy(), outcome))\n",
    "        return returnMemory\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batch_index in range(0, len(memory), config['BatchSize']):\n",
    "            sample = memory[batch_index:min(batch_index + config['BatchSize'], len(memory))]\n",
    "            state, probs, outcome = zip(*sample)\n",
    "            state = torch.stack(state).to(device).float()\n",
    "            policy_target = torch.stack(probs).to(device).float()\n",
    "            value_target = torch.tensor(outcome, dtype=torch.float, device=device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # forward pass\n",
    "            actor, critic = self.model(state)\n",
    "\n",
    "            actor_loss = F.kl_div(F.log_softmax(actor, dim=1), policy_target, reduction='batchmean')\n",
    "            critic_loss = F.mse_loss(critic.squeeze(1), value_target)\n",
    "\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in trange(config['Iterations']):\n",
    "            memory = []\n",
    "\n",
    "            model.eval()\n",
    "            for game in range(config['GamesPerIteration']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            model.train()\n",
    "            for epoch in range(config['Epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            timestamp = DateTimeGet()\n",
    "            torch.save(self.model.state_dict(), f\"model_{timestamp}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{timestamp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Nexus_Small()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['LearningRate'])\n",
    "agent = Nexus_Agent(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\js314\\AppData\\Local\\Temp\\ipykernel_29416\\4209454263.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  policy_TNSR = torch.tensor(policy, dtype=torch.float, device=device)\n",
      "  0%|          | 0/10 [02:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mNexus_Agent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGamesPerIteration\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 62\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mNexus_Agent.selfPlay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m board \u001b[38;5;241m=\u001b[39m OmokGame()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     memory\u001b[38;5;241m.\u001b[39mappend((board\u001b[38;5;241m.\u001b[39mget_board(), action_probs, board\u001b[38;5;241m.\u001b[39mget_turn()))\n\u001b[0;32m     16\u001b[0m     policy \u001b[38;5;241m=\u001b[39m action_probs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMPERATURE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mMCTS.search\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gameResult \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     actor, critic \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_board\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(actor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     38\u001b[0m     valid_moves \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_legal_moves()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mNexus_Small.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mActor(x), F\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCritic(x))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\pooling.py:1267\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:1260\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[0;32m   1259\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_output_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
